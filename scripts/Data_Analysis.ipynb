{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning completed successfully! Cleaned file saved at: C:\\Users\\gaura\\OneDrive\\Documents\\FederatedLearning_Project\\differential-privacy-fl\\data\\cleaned_chartevents.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "input_path = r\"C:\\Users\\gaura\\OneDrive\\Documents\\FederatedLearning_Project\\differential-privacy-fl\\data\\chartevents.csv\"\n",
    "output_path = r\"C:\\Users\\gaura\\OneDrive\\Documents\\FederatedLearning_Project\\differential-privacy-fl\\data\\cleaned_chartevents.csv\"\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(input_path):\n",
    "    raise FileNotFoundError(f\"Error: Input file not found at {input_path}\")\n",
    "\n",
    "# Define required itemids\n",
    "required_itemids = {220621, 220210, 220277, 223761, 220051, 220045, 220050}\n",
    "\n",
    "# Define chunk size (adjust based on available memory)\n",
    "chunk_size = 500000  # Process 500,000 rows at a time\n",
    "\n",
    "# Open the output file and write headers first\n",
    "header_written = False\n",
    "\n",
    "# Read CSV in chunks\n",
    "for chunk in pd.read_csv(input_path, usecols=[\"subject_id\", \"itemid\", \"valuenum\"], chunksize=chunk_size):\n",
    "    # Filter required itemids and remove rows with NaN values in 'valuenum'\n",
    "    chunk_filtered = chunk[chunk[\"itemid\"].isin(required_itemids)].dropna(subset=[\"valuenum\"])\n",
    "\n",
    "    # Append to the output file (write header only for the first chunk)\n",
    "    chunk_filtered.to_csv(output_path, mode=\"a\", index=False, header=not header_written)\n",
    "    \n",
    "    # Set flag to avoid writing header again\n",
    "    header_written = True\n",
    "\n",
    "print(\"Data cleaning completed successfully! Cleaned file saved at:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivoting completed successfully! Pivoted file saved at: C:\\Users\\gaura\\OneDrive\\Documents\\FederatedLearning_Project\\differential-privacy-fl\\data\\pivoted_chartevents.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "input_path = r\"C:\\Users\\gaura\\OneDrive\\Documents\\FederatedLearning_Project\\differential-privacy-fl\\data\\cleaned_chartevents.csv\"\n",
    "output_path = r\"C:\\Users\\gaura\\OneDrive\\Documents\\FederatedLearning_Project\\differential-privacy-fl\\data\\pivoted_chartevents.csv\"\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(input_path):\n",
    "    raise FileNotFoundError(f\"Error: Input file not found at {input_path}\")\n",
    "\n",
    "# Define mapping of itemid to feature names\n",
    "itemid_map = {\n",
    "    220621: \"Glucose\",\n",
    "    220210: \"Respiratory Rate\",\n",
    "    220277: \"O2 Saturation\",\n",
    "    223761: \"Temperature (Â°F)\",\n",
    "    220051: \"ABP Diastolic\",\n",
    "    220045: \"Heart Rate\",\n",
    "    220050: \"ABP Systolic\"\n",
    "}\n",
    "\n",
    "# Define chunk size for processing\n",
    "chunk_size = 500000  \n",
    "\n",
    "# Initialize a list to store processed chunks\n",
    "processed_chunks = []\n",
    "\n",
    "# Read and process CSV in chunks\n",
    "for chunk in pd.read_csv(input_path, chunksize=chunk_size):\n",
    "    # Replace 'itemid' with feature names\n",
    "    chunk[\"itemid\"] = chunk[\"itemid\"].map(itemid_map)  \n",
    "\n",
    "    # Pivot using pivot_table() with mean aggregation and rounding to 2 decimal places\n",
    "    chunk_pivoted = chunk.pivot_table(index=\"subject_id\", columns=\"itemid\", values=\"valuenum\", aggfunc=\"mean\").round(2)\n",
    "\n",
    "    # Append processed chunk\n",
    "    processed_chunks.append(chunk_pivoted)\n",
    "\n",
    "# Concatenate all chunks into a final dataframe\n",
    "df_final = pd.concat(processed_chunks)\n",
    "\n",
    "# Save the final pivoted dataframe to CSV\n",
    "df_final.to_csv(output_path, index=True)\n",
    "\n",
    "print(\"Pivoting completed successfully! Pivoted file saved at:\", output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
